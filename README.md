#rdfia #tme9 #cnn-visualization #imagenet #pytorch

## Visualization of Neural Networks

In this practical work, we experiment with diverse techniques with the goal to study the behavior of convolutional neural networks. 

The techniques we study are:
- Salienty maps: visualization of the importance of each pixel in the image to the classification score of the true class. (Simonyan et al. (2014)](https://arxiv.org/abs/1312.6034))
- Adversarial examples / fooling examples: adding minor modifications to an image, imperceptible to a human, which lead to a wrong classification of the image. ([Szegedy et al. (2014)](https://arxiv.org/abs/1312.6199), [Goodfellow et al. (2015)](https://arxiv.org/abs/1412.6572))
- Class visualization (Deep Dream-like): "generation" of an image corresponding to a category, in order to visualize the type of patterns detected by the network. (Simonyan et al. (2014), (Yosinski et al. (2015))[https://arxiv.org/abs/1506.06579])
